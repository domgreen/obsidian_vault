---
id: "20240312"
aliases: []
tags: []
---

# 20240312

- [x] Catchup with #michael_donoghue
- [x] Daily #leetcode [1171 Remove Zero Sum Consecutive Nodes](https://leetcode.com/problems/remove-zero-sum-consecutive-nodes-from-linked-list/description/)
  - this one was actually quite hard
  - used a technique called previous sum
- [x] Book in follow up with #alphasense
- [x] Read #system_design section of CTCI
  - wasn't actually a section for this
  - Watched this video [How to Pass Any System Design Interview](https://www.youtube.com/watch?v=o-k7h2G3Gco)
    - Good playlist here [System Design Interviews](https://www.youtube.com/playlist?list=PLCRMIe5FDPseVvwzRiCQBmNOVUIZSSkP8)
    - Practice 
      - sketch it out 
        - main flows
      - how do things each join together
      - any chaching / dbs
    - Look at common design pattersn
      - CDNs
      - load balancers
    - Get proficient with the tools
      - Excalidraw
    - Requirements Gathering - Ask Targetted Questions
      - Features
      - Non-Functional Requirements
      - Scalability
      - Technical Constraints
      - Important Numbers
      - Use Cases
      - Scoping
    - Back of The Envelope Estimations
    - Diagramming 
      - High level design
      - talk out loud
      - discuss rationale
      - tradeoffs
      - descisions
    - #consistent_hashing
      - distribute the data over servers to get consitent access
      - use a hashing function to a known range of values
        - good hash is disstributed over the range
      - simple hashing might not be good enough when the number of nodes change
      - Consistent hashing will mitigate these issues
        - hash the object and servers
        - hash teh servers by IP onto a ring
        - hash each objects by the keys
        - go clockwise till you find the next server
        - adding a new server will mean that you need to change where some objects are stored
      - *simple*
        - needs all keys to be remapped
      - *consistent*
        - only need to move a subset of the data
      - virtual nodes are used to keep spreading the data better
        - balances the data better
   - #bloom_filter
     - checks if an element in the set
     - false positive are possible
     - trade off
       - uses less memory
       - but is probablalistic
       - use if false positives are okay
       - cannot remove items in a bloom filter
     - hash function
       - even and random distributions
     - large set of buckets containng a single bit
     - when adding items use the hash to add 1's to bucket
     - check the new value against the bloom filter
     - use size based on the number of items that will be checked
   - [Calculations](https://www.youtube.com/watch?v=UC5xf8FbdJc&list=PLCRMIe5FDPseVvwzRiCQBmNOVUIZSSkP8&index=4)
     - request per second (web server)
	       - DAU
	       - usage per DAU
	         - eg. only 10-20% on twitter will post
	         - therefore read heavy
		 - peak usage
	 - Example (Tweets per day)
		 - 150M DAU
		 - peak is 2x
		 - Tweets per second
			 - 150M (DAU) * 0.5 tweets/DAU (number of tweets per DAU) * 2 (scaling) / seconds in a day
			 - Convert to scientific numbers
			 - 1.5 x 10^8   * 0.5 * 2 / 1 * 10^5
			 - 1.5 * 0.5 * 2 = 1.5
			 - 10^(8-5) = 10^3
			 - 1.5 * 10^3
			 - 1500 TPS
 - Why is #kafka fast? [video](https://www.youtube.com/watch?v=UNUz1-msbOM)
	 - Sequencial Disk Reads
	 - zero copy memory
 - [20 System Design Concepts Explained in 10 Minutes](https://www.youtube.com/watch?v=i53Gi_K3o7I)
- [x] Update meeting with #companiongroup
- [x] Another #leetcode question [Number of 1 bits](https://leetcode.com/problems/number-of-1-bits/)
- [x] Watch a #system_design video
- [ ] Apply to jobs
